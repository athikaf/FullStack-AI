{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU1_cyNuScqb",
        "outputId": "67ea990d-7e98-46ba-9b24-2814bf12ef1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install all dependencies\n",
        "!pip install -q requests pandas numpy plotly streamlit pyngrok cloudflared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "VB_HQghoSsCE"
      },
      "outputs": [],
      "source": [
        "# Cell 1 — imports, constants, folders\n",
        "import os, json, time, pathlib, requests                       # std + HTTP + filesystem\n",
        "import pandas as pd, numpy as np                               # data wrangling\n",
        "from datetime import datetime                                  # timestamps for filenames\n",
        "\n",
        "# Primary & secondary APIs\n",
        "BASE_URL_CG = \"https://api.coingecko.com/api/v3\"               # main source\n",
        "BASE_URL_PAPRIKA = \"https://api.coinpaprika.com/v1\"            # backup source (no key)\n",
        "\n",
        "# Tunables\n",
        "MAX_COINS = 30                                                 # how many coins to fetch/analyze\n",
        "TIMEOUT   = 8                                                  # HTTP timeout (seconds)\n",
        "REQUEST_DELAY = 2.0                                            # polite delay after live calls\n",
        "CACHE_TTL = 600                                                # cache freshness (seconds)\n",
        "\n",
        "# Folders\n",
        "CACHE_DIR   = pathlib.Path(\"./cache\");   CACHE_DIR.mkdir(exist_ok=True)     # on-disk json cache\n",
        "OFFLINE_DIR = pathlib.Path(\"./offline\"); OFFLINE_DIR.mkdir(exist_ok=True)   # snapshots & fallbacks\n",
        "\n",
        "# Offline files\n",
        "OFFLINE_FILE     = OFFLINE_DIR / \"markets_sample.json\"                       # our JSON snapshot\n",
        "KAGGLE_FALLBACK  = OFFLINE_DIR / \"kaggle_markets.csv\"                        # optional CSV fallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "LV3r1b7mYd2G"
      },
      "outputs": [],
      "source": [
        "# Cell 2 — cache helpers + %change util\n",
        "def _cache_path(key: str) -> pathlib.Path:\n",
        "    return CACHE_DIR / f\"{key}.json\"                                        # path for a cached response\n",
        "\n",
        "def _read_cache(key: str, ttl: int = CACHE_TTL):\n",
        "    p = _cache_path(key)\n",
        "    if not p.exists(): return None                                          # nothing cached\n",
        "    if time.time() - p.stat().st_mtime > ttl: return None                   # stale\n",
        "    try:\n",
        "        return json.loads(p.read_text())                                    # parse JSON\n",
        "    except Exception:\n",
        "        return None                                                         # bad cache → ignore\n",
        "\n",
        "def _write_cache(key: str, data):\n",
        "    _cache_path(key).write_text(json.dumps(data))                           # save JSON\n",
        "\n",
        "def pct_change_from_sparkline(prices, hours: int):\n",
        "    \"\"\"Compute % change over `hours` using the sparkline (hourly).\"\"\"\n",
        "    if not prices or len(prices) <= hours: return None\n",
        "    last = float(prices[-1]); prev = float(prices[-hours-1])                # ~hours points back\n",
        "    if prev == 0: return None\n",
        "    return 100.0 * (last - prev) / prev\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1b2NIOS3Yech"
      },
      "outputs": [],
      "source": [
        "# Cell 3 — CoinGecko (primary) + normalizer\n",
        "def fetch_cg_markets(max_coins=MAX_COINS, currency=\"usd\", ttl=CACHE_TTL):\n",
        "    key = f\"cg_{currency}_{max_coins}_spark\"                                # cache key includes sparkline\n",
        "    cached = _read_cache(key, ttl=ttl)\n",
        "    if cached is not None:\n",
        "        return cached, \"CACHE(CG)\"                                          # instant return from cache\n",
        "\n",
        "    url = (f\"{BASE_URL_CG}/coins/markets?vs_currency={currency}\"\n",
        "           f\"&order=market_cap_desc&per_page={max_coins}&page=1\"\n",
        "           f\"&sparkline=true&price_change_percentage=1h,24h,7d,30d\")        # sparkline ON\n",
        "    r = requests.get(url, timeout=TIMEOUT, headers={\"Accept\": \"application/json\"})\n",
        "    if r.status_code != 200:\n",
        "        raise RuntimeError(f\"CG {r.status_code}\")                           # bubble up to fallback\n",
        "    data = r.json()\n",
        "    _write_cache(key, data)                                                 # cache for next call\n",
        "    time.sleep(REQUEST_DELAY)                                               # be polite to free API\n",
        "    return data, \"LIVE(CG)\"\n",
        "\n",
        "def norm_from_cg(row: dict) -> dict:\n",
        "    \"\"\"Map CoinGecko fields → our notebook schema.\"\"\"\n",
        "    return {\n",
        "        \"id\": row.get(\"id\"),\n",
        "        \"name\": row.get(\"name\"),\n",
        "        \"symbol\": (row.get(\"symbol\") or \"\").upper(),\n",
        "        \"current_price\": row.get(\"current_price\"),\n",
        "        \"market_cap\": row.get(\"market_cap\"),\n",
        "        \"volume_24h\": row.get(\"total_volume\"),                              # align naming\n",
        "        \"price_change_percentage_1h\":  row.get(\"price_change_percentage_1h_in_currency\", row.get(\"price_change_percentage_1h\")),\n",
        "        \"price_change_percentage_24h\": row.get(\"price_change_percentage_24h_in_currency\", row.get(\"price_change_percentage_24h\")),\n",
        "        \"price_change_percentage_7d\":  row.get(\"price_change_percentage_7d_in_currency\", row.get(\"price_change_percentage_7d\")),\n",
        "        \"price_change_percentage_30d\": row.get(\"price_change_percentage_30d_in_currency\", row.get(\"price_change_percentage_30d\")),\n",
        "        \"sparkline\": (row.get(\"sparkline\") or {}).get(\"price\", []),         # hourly prices (~7d)\n",
        "    }\n",
        "\n",
        "# Cell 3b — CoinPaprika (secondary) + normalizer\n",
        "def fetch_paprika_tickers(max_coins=MAX_COINS, ttl=CACHE_TTL):\n",
        "    key = f\"paprika_{max_coins}\"\n",
        "    cached = _read_cache(key, ttl=ttl)\n",
        "    if cached is not None:\n",
        "        return cached, \"CACHE(PAPRIKA)\"\n",
        "    url = f\"{BASE_URL_PAPRIKA}/tickers?quotes=USD\"\n",
        "    r = requests.get(url, timeout=TIMEOUT, headers={\"Accept\": \"application/json\"})\n",
        "    if r.status_code != 200:\n",
        "        raise RuntimeError(f\"PAP {r.status_code}\")\n",
        "    data = r.json()\n",
        "    _write_cache(key, data)\n",
        "    time.sleep(REQUEST_DELAY)\n",
        "    return data, \"LIVE(PAPRIKA)\"\n",
        "\n",
        "def norm_from_paprika(row: dict) -> dict:\n",
        "    \"\"\"Map Paprika fields → our schema (no sparkline here).\"\"\"\n",
        "    q = (row.get(\"quotes\") or {}).get(\"USD\", {})\n",
        "    return {\n",
        "        \"id\": row.get(\"id\"),\n",
        "        \"name\": row.get(\"name\"),\n",
        "        \"symbol\": (row.get(\"symbol\") or \"\").upper(),\n",
        "        \"current_price\": q.get(\"price\"),\n",
        "        \"market_cap\": q.get(\"market_cap\"),\n",
        "        \"volume_24h\": q.get(\"volume_24h\"),\n",
        "        \"price_change_percentage_1h\":  q.get(\"percent_change_1h\"),\n",
        "        \"price_change_percentage_24h\": q.get(\"percent_change_24h\"),\n",
        "        \"price_change_percentage_7d\":  q.get(\"percent_change_7d\"),\n",
        "        \"price_change_percentage_30d\": None,\n",
        "        \"sparkline\": [],                                                    # we’ll enrich later if needed\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "3jQB8lbEYhFi"
      },
      "outputs": [],
      "source": [
        "# Cell 4 — offline & Kaggle helpers + CDD enrichment for 48/72\n",
        "def save_offline_snapshot(rows_norm: list):\n",
        "    \"\"\"Write normalized rows to offline JSON (best-effort).\"\"\"\n",
        "    try:\n",
        "        OFFLINE_FILE.write_text(json.dumps(rows_norm, indent=2))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def load_offline_json() -> list:\n",
        "    \"\"\"Read normalized rows from offline JSON, if present.\"\"\"\n",
        "    if OFFLINE_FILE.exists():\n",
        "        try:\n",
        "            return json.loads(OFFLINE_FILE.read_text())\n",
        "        except Exception:\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "def load_kaggle_csv() -> pd.DataFrame:\n",
        "    \"\"\"Optional: load a pre-normalized CSV fallback.\"\"\"\n",
        "    if not KAGGLE_FALLBACK.exists(): return pd.DataFrame()\n",
        "    try:\n",
        "        return pd.read_csv(KAGGLE_FALLBACK)\n",
        "    except Exception as e:\n",
        "        print(\"Kaggle CSV read error:\", e)\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Minimal map of symbols→Binance pairs for CryptoDataDownload\n",
        "CDD_MAP = {\n",
        "    \"BTC\":\"BTCUSDT\",\"ETH\":\"ETHUSDT\",\"BNB\":\"BNBUSDT\",\"SOL\":\"SOLUSDT\",\"XRP\":\"XRPUSDT\",\n",
        "    \"ADA\":\"ADAUSDT\",\"DOGE\":\"DOGEUSDT\",\"TON\":\"TONUSDT\",\"SHIB\":\"SHIBUSDT\",\"LTC\":\"LTCUSDT\",\n",
        "    \"TRX\":\"TRXUSDT\",\"AVAX\":\"AVAXUSDT\",\n",
        "}\n",
        "\n",
        "def fetch_cdd_pct_changes(symbol: str):\n",
        "    \"\"\"Best-effort: get 24/48/72h % change from CDD hourly CSV if API lacks sparkline.\"\"\"\n",
        "    pair = CDD_MAP.get(symbol.upper())\n",
        "    if not pair: return None, None, None\n",
        "    url = f\"https://www.cryptodatadownload.com/cdd/Binance_{pair}_1h.csv\"\n",
        "    try:\n",
        "        text = requests.get(url, timeout=TIMEOUT).text\n",
        "        lines = [ln for ln in text.splitlines() if not ln.startswith(\"#\")]         # skip comments\n",
        "        if len(lines) < 10: return None, None, None\n",
        "        from io import StringIO\n",
        "        df = pd.read_csv(StringIO(\"\\n\".join(lines)))\n",
        "        close_col = \"close\" if \"close\" in df.columns else (\"Close\" if \"Close\" in df.columns else None)\n",
        "        if close_col is None: return None, None, None\n",
        "        closes = df[close_col].astype(float).tolist()[::-1]                         # oldest→newest\n",
        "        c24 = pct_change_from_sparkline(closes, 24)\n",
        "        c48 = pct_change_from_sparkline(closes, 48)\n",
        "        c72 = pct_change_from_sparkline(closes, 72)\n",
        "        return c24, c48, c72\n",
        "    except Exception:\n",
        "        return None, None, None\n",
        "\n",
        "def enrich_48_72_with_cdd(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Fill 48/72 (and 24 if missing) via CDD for common symbols.\"\"\"\n",
        "    if df.empty: return df\n",
        "    df = df.copy()\n",
        "    if \"price_change_percentage_48h\" not in df: df[\"price_change_percentage_48h\"] = np.nan\n",
        "    if \"price_change_percentage_72h\" not in df: df[\"price_change_percentage_72h\"] = np.nan\n",
        "    need = df[\"price_change_percentage_48h\"].isna() | df[\"price_change_percentage_72h\"].isna()\n",
        "    for i, r in df[need].iterrows():\n",
        "        c24, c48, c72 = fetch_cdd_pct_changes(r[\"symbol\"])\n",
        "        if c24 is not None and pd.isna(r.get(\"price_change_percentage_24h\", np.nan)):\n",
        "            df.at[i, \"price_change_percentage_24h\"] = c24\n",
        "        if c48 is not None: df.at[i, \"price_change_percentage_48h\"] = c48\n",
        "        if c72 is not None: df.at[i, \"price_change_percentage_72h\"] = c72\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "NRwZDzbCYiyT"
      },
      "outputs": [],
      "source": [
        "# Cell 5 — unified loader: CG → Paprika → Offline → Kaggle\n",
        "def get_markets_multi(source=\"auto\", max_coins=MAX_COINS, currency=\"usd\"):\n",
        "    order = [\"coingecko\",\"paprika\",\"offline\",\"kaggle\"] if source==\"auto\" else [source]\n",
        "    last_err = None\n",
        "    for src in order:\n",
        "        try:\n",
        "            if src == \"coingecko\":\n",
        "                rows, src_used = fetch_cg_markets(max_coins, currency)       # live or cache\n",
        "                norm = [norm_from_cg(r) for r in rows][:max_coins]           # normalize rows\n",
        "                save_offline_snapshot(norm)                                  # refresh offline\n",
        "                df = pd.DataFrame(norm)\n",
        "\n",
        "                # Compute 24/48/72 from sparkline\n",
        "                df[\"price_change_percentage_48h\"] = df[\"sparkline\"].apply(lambda p: pct_change_from_sparkline(p, 48))\n",
        "                df[\"price_change_percentage_72h\"] = df[\"sparkline\"].apply(lambda p: pct_change_from_sparkline(p, 72))\n",
        "                df[\"price_change_percentage_24h\"] = df.apply(\n",
        "                    lambda r: r[\"price_change_percentage_24h\"] if pd.notna(r[\"price_change_percentage_24h\"])\n",
        "                             else pct_change_from_sparkline(r[\"sparkline\"], 24), axis=1\n",
        "                )\n",
        "                return df.head(max_coins), src_used\n",
        "\n",
        "            if src == \"paprika\":\n",
        "                rows, src_used = fetch_paprika_tickers(max_coins)            # live or cache\n",
        "                norm = [norm_from_paprika(r) for r in rows][:max_coins]\n",
        "                df = pd.DataFrame(norm)\n",
        "                df = enrich_48_72_with_cdd(df)                               # fill 48/72 if possible\n",
        "                return df.head(max_coins), src_used\n",
        "\n",
        "            if src == \"offline\":\n",
        "                raw = load_offline_json()                                    # load snapshot if exists\n",
        "                if raw:\n",
        "                    df = pd.DataFrame(raw).head(max_coins)\n",
        "                    if \"sparkline\" in df.columns:\n",
        "                        df[\"price_change_percentage_48h\"] = df[\"sparkline\"].apply(lambda p: pct_change_from_sparkline(p, 48))\n",
        "                        df[\"price_change_percentage_72h\"] = df[\"sparkline\"].apply(lambda p: pct_change_from_sparkline(p, 72))\n",
        "                    return df, \"OFFLINE(JSON)\"\n",
        "\n",
        "            if src == \"kaggle\":\n",
        "                kag = load_kaggle_csv()                                      # final fallback\n",
        "                if not kag.empty:\n",
        "                    return kag.head(max_coins), \"OFFLINE(KAGGLE)\"\n",
        "\n",
        "        except Exception as e:\n",
        "            last_err = e                                                     # remember and try next\n",
        "            continue\n",
        "\n",
        "    raise RuntimeError(f\"All sources failed: {last_err}\")                    # bubble up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ESI3EfJ5YkyI"
      },
      "outputs": [],
      "source": [
        "# Cell 6 — feature helpers (same names; safe to reuse elsewhere)\n",
        "\n",
        "def calculate_volatility_proxy(coin_row: dict):\n",
        "    \"\"\"Mean absolute % moves across 1h, 24h, 7d (simple volatility proxy).\"\"\"\n",
        "    vals = []\n",
        "    for k in [\"price_change_percentage_1h\",\"price_change_percentage_24h\",\"price_change_percentage_7d\"]:\n",
        "        v = coin_row.get(k)\n",
        "        if v is not None: vals.append(abs(v))\n",
        "    return None if not vals else float(np.mean(vals))\n",
        "\n",
        "def simple_trend_forecast(coin_row: dict):\n",
        "    \"\"\"Toy forecast: nudge current price by 30% of (0.7*24h + 0.3*7d). Demo only.\"\"\"\n",
        "    c24 = coin_row.get(\"price_change_percentage_24h\", 0) or 0\n",
        "    c7  = coin_row.get(\"price_change_percentage_7d\", 0)  or 0\n",
        "    cur = coin_row.get(\"current_price\", 0) or 0\n",
        "    if not cur: return None\n",
        "    trend = 0.7*c24 + 0.3*c7\n",
        "    return round(cur * (1 + 0.3*(trend/100)), 6)\n",
        "\n",
        "def compute_risk_score(row: dict):\n",
        "    \"\"\"Absolute rule score out of 100 + reason (independent from normalized risk).\"\"\"\n",
        "    score, reasons = 0, []\n",
        "    if row.get(\"price_change_percentage_24h\") is not None and row[\"price_change_percentage_24h\"] < -2:\n",
        "        score += 50; reasons.append(\"24h drop > 2%\")\n",
        "    if row.get(\"volatility_proxy\") is not None and row[\"volatility_proxy\"] > 3:\n",
        "        score += 30; reasons.append(\"High recent volatility\")\n",
        "    if row.get(\"volume_24h\") and row.get(\"market_cap\") and row[\"volume_24h\"] > 0.5*row[\"market_cap\"]:\n",
        "        score += 15; reasons.append(\"Heavy 24h volume vs market cap\")\n",
        "    if row.get(\"price_change_percentage_1h\") is not None and row[\"price_change_percentage_1h\"] < -0.5:\n",
        "        score += 20; reasons.append(\"1h downside\")\n",
        "    if score == 0: reasons.append(\"Stable / normal range\")\n",
        "    return score, \"; \".join(reasons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "a2fbWOXwSz8n"
      },
      "outputs": [],
      "source": [
        "# Cell 7 — main analysis pipeline\n",
        "def analyze_free_data(source=\"auto\"):\n",
        "    \"\"\"\n",
        "    1) Load markets via multi-source fallback\n",
        "    2) Assemble per-coin dicts with raw + derived features\n",
        "    3) Clean dtypes\n",
        "    4) Compute normalized cohort risk (0–100)\n",
        "    5) Human-readable reasons\n",
        "    \"\"\"\n",
        "    # 1) fetch with fallbacks\n",
        "    market_df, source_used = get_markets_multi(source=source, max_coins=MAX_COINS, currency=\"usd\")\n",
        "    print(f\"[DATA SOURCE] {source_used} ✓\")\n",
        "    if market_df is None or market_df.empty:\n",
        "        print(\"No market data available.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Convert to records so we can reuse existing row logic easily\n",
        "    records = market_df.to_dict(\"records\")\n",
        "\n",
        "    # 2) per-coin features\n",
        "    results = []\n",
        "    for coin in records:\n",
        "        # core fields (safe gets)\n",
        "        coin_id = coin.get(\"id\")\n",
        "        name    = coin.get(\"name\")\n",
        "        symbol  = (coin.get(\"symbol\") or \"\").upper()\n",
        "        price   = coin.get(\"current_price\")\n",
        "        mcap    = coin.get(\"market_cap\", 0)\n",
        "        vol24   = coin.get(\"volume_24h\", 0)\n",
        "\n",
        "        # % changes (API + sparkline/CDD backfills already done in loader)\n",
        "        c1   = coin.get(\"price_change_percentage_1h\")\n",
        "        c24  = coin.get(\"price_change_percentage_24h\")\n",
        "        c48  = coin.get(\"price_change_percentage_48h\")\n",
        "        c72  = coin.get(\"price_change_percentage_72h\")\n",
        "        c7d  = coin.get(\"price_change_percentage_7d\")\n",
        "        c30d = coin.get(\"price_change_percentage_30d\")\n",
        "\n",
        "        # volatility proxy + toy forecast\n",
        "        vol_proxy = calculate_volatility_proxy(coin)\n",
        "        forecast  = simple_trend_forecast(coin)\n",
        "\n",
        "        # collect row\n",
        "        results.append({\n",
        "            \"coin_id\": coin_id, \"name\": name, \"symbol\": symbol,\n",
        "            \"current_price\": price, \"market_cap\": mcap, \"volume_24h\": vol24,\n",
        "            \"price_change_percentage_1h\": c1,\n",
        "            \"price_change_percentage_24h\": c24,\n",
        "            \"price_change_percentage_48h\": c48,\n",
        "            \"price_change_percentage_72h\": c72,\n",
        "            \"price_change_percentage_7d\": c7d,\n",
        "            \"price_change_percentage_30d\": c30d,\n",
        "            \"volatility_proxy\": vol_proxy, \"trend_forecast\": forecast,\n",
        "        })\n",
        "\n",
        "    # 3) DataFrame + type coercion\n",
        "    df = pd.DataFrame(results)\n",
        "    for col in [\n",
        "        \"current_price\",\"market_cap\",\"volume_24h\",\n",
        "        \"price_change_percentage_1h\",\"price_change_percentage_24h\",\n",
        "        \"price_change_percentage_48h\",\"price_change_percentage_72h\",\n",
        "        \"price_change_percentage_7d\",\"price_change_percentage_30d\",\n",
        "        \"volatility_proxy\",\"trend_forecast\"\n",
        "    ]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    # 4) normalized risk components (sigmoid of z-scores)\n",
        "    df[\"abs_24h\"]     = df[\"price_change_percentage_24h\"].abs()\n",
        "    df[\"down_1h\"]     = df[\"price_change_percentage_1h\"].clip(upper=0).abs()\n",
        "    df[\"volume_ratio\"]= (df[\"volume_24h\"]/df[\"market_cap\"]).clip(lower=0, upper=1)\n",
        "\n",
        "    def _norm(s: pd.Series) -> pd.Series:\n",
        "        s2 = s.replace([np.inf,-np.inf], np.nan)\n",
        "        m, sd = s2.mean(), s2.std()\n",
        "        if not np.isfinite(sd) or sd == 0: return pd.Series(0, index=s.index)\n",
        "        z = (s2 - m) / sd\n",
        "        return 1/(1+np.exp(-z))\n",
        "\n",
        "    risk = (0.50*_norm(df[\"volatility_proxy\"]) +\n",
        "            0.30*_norm(df[\"abs_24h\"]) +\n",
        "            0.15*_norm(df[\"down_1h\"]) +\n",
        "            0.05*_norm(df[\"volume_ratio\"]))\n",
        "    df[\"risk_score\"] = (100*risk).clip(0,100).round(1)\n",
        "\n",
        "    # 5) reasons\n",
        "    reasons = []\n",
        "    for _, r in df.iterrows():\n",
        "        why=[]\n",
        "        if r[\"volatility_proxy\"] > df[\"volatility_proxy\"].median(): why.append(\"High recent volatility\")\n",
        "        if r[\"abs_24h\"]          > df[\"abs_24h\"].median():         why.append(\"Large 24h move\")\n",
        "        if r[\"price_change_percentage_1h\"] < -0.5:                  why.append(\"1h downside\")\n",
        "        if r[\"volume_ratio\"] > 0.5:                                 why.append(\"Heavy volume vs mcap\")\n",
        "        reasons.append(\"; \".join(why) if why else \"Stable / normal range\")\n",
        "    df[\"risk_reason\"] = reasons\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "dl7fKwTKS3tZ"
      },
      "outputs": [],
      "source": [
        "# Cell 8 — pretty console table\n",
        "def display_results(df: pd.DataFrame):\n",
        "    if df is None or df.empty:\n",
        "        print(\"No data to display.\"); return\n",
        "\n",
        "    df_sorted = df.sort_values(\"risk_score\", ascending=False)\n",
        "    header = f\"{'Name':<20} {'Sym':<6} {'Price($)':>10} {'24h%':>8} {'48h%':>8} {'72h%':>8} {'VolProxy':>10} {'Risk%':>7}  Reason\"\n",
        "    print(header)\n",
        "    print('-'*len(header))\n",
        "    for _, r in df_sorted.iterrows():\n",
        "        print(f\"{str(r['name'])[:20]:<20} {r['symbol']:<6} \"\n",
        "              f\"{(r['current_price'] or 0):>10.4f} \"\n",
        "              f\"{(r.get('price_change_percentage_24h') or 0):>8.2f} \"\n",
        "              f\"{(r.get('price_change_percentage_48h') or 0):>8.2f} \"\n",
        "              f\"{(r.get('price_change_percentage_72h') or 0):>8.2f} \"\n",
        "              f\"{(r['volatility_proxy'] or 0):>10.2f} \"\n",
        "              f\"{r['risk_score']:>7.1f}  \"\n",
        "              f\"{r['risk_reason']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ba2KDk6rS7sE"
      },
      "outputs": [],
      "source": [
        "# Cell 9 — Plotly charts (save to HTML; show inline if you want)\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def create_visualizations(df: pd.DataFrame):\n",
        "    if df is None or df.empty:\n",
        "        print(\"No data for charts.\"); return\n",
        "\n",
        "    # 1) Top-10 volatility\n",
        "    top_vol = df.nlargest(10, \"volatility_proxy\")\n",
        "    fig1 = px.bar(top_vol, x=\"symbol\", y=\"volatility_proxy\", title=\"Top-10 Volatility Proxy\")\n",
        "    fig1.write_html(\"volatility_proxy_chart.html\")\n",
        "\n",
        "    # 2) 24h vs 7d grouped\n",
        "    fig2 = go.Figure()\n",
        "    fig2.add_bar(name=\"24h %\", x=df[\"symbol\"], y=df[\"price_change_percentage_24h\"])\n",
        "    fig2.add_bar(name=\"7d %\",  x=df[\"symbol\"], y=df[\"price_change_percentage_7d\"])\n",
        "    fig2.update_layout(barmode=\"group\", title=\"Price Change: 24h vs 7d\")\n",
        "    fig2.write_html(\"price_changes_chart.html\")\n",
        "\n",
        "    # 3) 24/48/72 grouped (new)\n",
        "    figX = go.Figure()\n",
        "    figX.add_bar(name=\"24h %\", x=df[\"symbol\"], y=df[\"price_change_percentage_24h\"])\n",
        "    figX.add_bar(name=\"48h %\", x=df[\"symbol\"], y=df[\"price_change_percentage_48h\"])\n",
        "    figX.add_bar(name=\"72h %\", x=df[\"symbol\"], y=df[\"price_change_percentage_72h\"])\n",
        "    figX.update_layout(barmode=\"group\", title=\"Price Change: 24/48/72h\")\n",
        "    figX.write_html(\"price_changes_24_48_72.html\")\n",
        "\n",
        "    # 4) Market cap vs 24h volume (log-log)\n",
        "    fig3 = px.scatter(df, x=\"market_cap\", y=\"volume_24h\", log_x=True, log_y=True,\n",
        "                      hover_name=\"name\", title=\"Market Cap vs 24h Volume (log-log)\")\n",
        "    fig3.write_html(\"market_analysis_chart.html\")\n",
        "\n",
        "    # 5) Top-10 risk\n",
        "    top_risk = df.nlargest(10, \"risk_score\")\n",
        "    fig4 = px.bar(top_risk, x=\"symbol\", y=\"risk_score\", title=\"Top-10 Crash Risk\")\n",
        "    fig4.write_html(\"risk_score_chart.html\")\n",
        "\n",
        "    print(\"Saved charts: volatility_proxy_chart.html, price_changes_chart.html, price_changes_24_48_72.html, market_analysis_chart.html, risk_score_chart.html\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "aN2eKgkbS_F2"
      },
      "outputs": [],
      "source": [
        "# Cell 10 — CSV export\n",
        "def save_results(df: pd.DataFrame):\n",
        "    if df is None or df.empty:\n",
        "        return\n",
        "    df_export = df.copy().round(6)\n",
        "    filename = f\"crypto_analysis_{datetime.now():%Y%m%d_%H%M}.csv\"\n",
        "    df_export.to_csv(filename, index=False)\n",
        "    print(f\"Results saved → {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS2R8SxJTDdH",
        "outputId": "656e567c-6417-4f59-b97f-19c538fb0526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Crashcaster pipeline start ===\n",
            "[1/4] Fetch + analyze …\n",
            "[DATA SOURCE] LIVE(CG) ✓\n",
            "[2/4] Print table …\n",
            "Name                 Sym      Price($)     24h%     48h%     72h%   VolProxy   Risk%  Reason\n",
            "--------------------------------------------------------------------------------------------\n",
            "Chainlink            LINK      22.4600    -6.46      nan      nan       9.71    74.6  High recent volatility; Large 24h move\n",
            "Solana               SOL      193.6900    -5.74      nan      nan       5.67    69.3  High recent volatility; Large 24h move\n",
            "Cardano              ADA        0.9248    -4.71      nan      nan       7.30    65.1  High recent volatility; Large 24h move\n",
            "Dogecoin             DOGE       0.2268    -9.58      nan      nan       4.17    61.3  High recent volatility; Large 24h move\n",
            "Ethereum             ETH     4633.4600    -2.54      nan      nan       7.31    59.3  High recent volatility\n",
            "Stellar              XLM        0.4276    -7.85      nan      nan       4.38    59.1  High recent volatility; Large 24h move\n",
            "Hedera               HBAR       0.2510    -8.11      nan      nan       4.25    59.0  High recent volatility; Large 24h move\n",
            "Wrapped stETH        WSTETH  5601.7000    -2.56      nan      nan       7.35    58.7  High recent volatility\n",
            "Wrapped Beacon ETH   WBETH   4985.8300    -2.46      nan      nan       7.38    58.5  High recent volatility\n",
            "WETH                 WETH    4633.3000    -2.53      nan      nan       7.25    58.4  High recent volatility\n",
            "Lido Staked Ether    STETH   4623.0400    -2.47      nan      nan       7.26    58.2  High recent volatility\n",
            "Wrapped eETH         WEETH   4964.0200    -2.47      nan      nan       7.26    58.2  High recent volatility\n",
            "Hyperliquid          HYPE      48.4500     1.23      nan      nan       7.61    56.7  High recent volatility\n",
            "XRP                  XRP        3.1100    -6.66      nan      nan       4.22    56.2  High recent volatility; Large 24h move\n",
            "Sui                  SUI        3.7900    -7.94      nan      nan       3.13    54.7  Large 24h move\n",
            "Litecoin             LTC      122.0300    -8.20      nan      nan       2.96    54.0  Large 24h move\n",
            "Avalanche            AVAX      23.9600    -7.30      nan      nan       3.31    53.9  High recent volatility; Large 24h move\n",
            "Shiba Inu            SHIB       0.0000    -8.05      nan      nan       3.05    53.6  Large 24h move\n",
            "Bitcoin Cash         BCH      595.1400    -4.47      nan      nan       2.58    45.6  Large 24h move\n",
            "Toncoin              TON        3.4300    -4.05      nan      nan       2.29    41.8  Large 24h move\n",
            "Bitcoin              BTC    118797.0000    -3.96      nan      nan       1.88    39.8  Large 24h move\n",
            "Wrapped Bitcoin      WBTC   118822.0000    -3.71      nan      nan       1.73    38.6  Large 24h move\n",
            "LEO Token            LEO        9.4100     1.67      nan      nan       2.45    36.4  Stable / normal range\n",
            "BNB                  BNB      843.4500    -0.86      nan      nan       2.67    35.9  Stable / normal range\n",
            "TRON                 TRX        0.3618    -0.42      nan      nan       2.70    35.5  Stable / normal range\n",
            "Tether               USDT       1.0010     0.06      nan      nan       0.05    28.1  Heavy volume vs mcap\n",
            "Binance Bridged USDT BSC-USD     1.0010     0.09      nan      nan       0.11    27.2  Stable / normal range\n",
            "Ethena USDe          USDE       1.0020     0.13      nan      nan       0.07    27.0  Stable / normal range\n",
            "USDC                 USDC       0.9998    -0.00      nan      nan       0.00    26.3  Stable / normal range\n",
            "USDS                 USDS       0.9999     0.03      nan      nan       0.03    24.8  Stable / normal range\n",
            "[3/4] Charts …\n",
            "Saved charts: volatility_proxy_chart.html, price_changes_chart.html, price_changes_24_48_72.html, market_analysis_chart.html, risk_score_chart.html\n",
            "[4/4] Save CSV …\n",
            "Results saved → crypto_analysis_20250814_2254.csv\n",
            "=== Done ===\n"
          ]
        }
      ],
      "source": [
        "# Cell 11 — main orchestration with logs\n",
        "def main(source=\"auto\"):\n",
        "    print(\"=== Crashcaster pipeline start ===\")\n",
        "    try:\n",
        "        print(\"[1/4] Fetch + analyze …\")\n",
        "        df = analyze_free_data(source=source)\n",
        "        if df is None or df.empty:\n",
        "            print(\"!! Empty DataFrame.\"); return df\n",
        "\n",
        "        print(\"[2/4] Print table …\")\n",
        "        try: display_results(df)\n",
        "        except Exception as e: print(\"display_results error:\", e)\n",
        "\n",
        "        print(\"[3/4] Charts …\")\n",
        "        try: create_visualizations(df)\n",
        "        except Exception as e: print(\"create_visualizations error:\", e)\n",
        "\n",
        "        print(\"[4/4] Save CSV …\")\n",
        "        try: save_results(df)\n",
        "        except Exception as e: print(\"save_results error:\", e)\n",
        "\n",
        "        print(\"=== Done ===\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        import traceback; traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Run once to populate `results_df`\n",
        "results_df = main(source=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD0tANxoVOWi",
        "outputId": "9cd7bbbf-2aa4-4b25-c5d4-bb682960b258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Recommended (within your volatility tolerance):\n",
            "Name                 Sym     VolProxy price_change_percentage_24h  Reason\n",
            "-------------------------------------------------------------------------\n",
            "USDC                 USDC        0.00        -0.00  Stable / normal range\n",
            "USDS                 USDS        0.03         0.03  Stable / normal range\n",
            "Tether               USDT        0.05         0.06  Heavy volume vs mcap\n",
            "Ethena USDe          USDE        0.07         0.13  Stable / normal range\n",
            "Binance Bridged USDT BSC-USD      0.11         0.09  Stable / normal range\n",
            "Wrapped Bitcoin      WBTC        1.73        -3.71  Large 24h move\n",
            "Bitcoin              BTC         1.88        -3.96  Large 24h move\n",
            "Toncoin              TON         2.29        -4.05  Large 24h move\n",
            "LEO Token            LEO         2.45         1.67  Stable / normal range\n",
            "Bitcoin Cash         BCH         2.58        -4.47  Large 24h move\n",
            "BNB                  BNB         2.67        -0.86  Stable / normal range\n",
            "TRON                 TRX         2.70        -0.42  Stable / normal range\n",
            "Litecoin             LTC         2.96        -8.20  Large 24h move\n",
            "Shiba Inu            SHIB        3.05        -8.05  Large 24h move\n",
            "Sui                  SUI         3.13        -7.94  Large 24h move\n",
            "Avalanche            AVAX        3.31        -7.30  High recent volatility; Large 24h move\n",
            "Dogecoin             DOGE        4.17        -9.58  High recent volatility; Large 24h move\n",
            "XRP                  XRP         4.22        -6.66  High recent volatility; Large 24h move\n",
            "Hedera               HBAR        4.25        -8.11  High recent volatility; Large 24h move\n",
            "Stellar              XLM         4.38        -7.85  High recent volatility; Large 24h move\n",
            "Solana               SOL         5.67        -5.74  High recent volatility; Large 24h move\n"
          ]
        }
      ],
      "source": [
        "# Cell 12 — risk tools + horizon mapping\n",
        "risk_level = 60                                           # demo default\n",
        "time_horizon = \"24h\"                                      # \"24h\" | \"48h\" | \"72h\" | \"7d\"\n",
        "\n",
        "def risk_to_volatility_threshold(risk_level: int) -> float:\n",
        "    risk_level = max(10, min(100, risk_level))            # clamp\n",
        "    return 1 + (risk_level - 10) * (9 / 90.0)             # linear 1..10\n",
        "\n",
        "def get_change_column(time_horizon: str) -> str:\n",
        "    mapping = {\n",
        "        \"24h\": \"price_change_percentage_24h\",\n",
        "        \"48h\": \"price_change_percentage_48h\",\n",
        "        \"72h\": \"price_change_percentage_72h\",\n",
        "        \"7d\":  \"price_change_percentage_7d\",\n",
        "    }\n",
        "    return mapping.get(time_horizon, \"price_change_percentage_24h\")\n",
        "\n",
        "def show_risk_based_recommendations(df: pd.DataFrame, risk_level: int, time_horizon: str):\n",
        "    \"\"\"Filter coins by volatility tolerance and show chosen horizon column.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        print(\"No data for recommendations.\"); return\n",
        "    vol_thresh = risk_to_volatility_threshold(risk_level)\n",
        "    col = get_change_column(time_horizon)\n",
        "    dff = df[df[\"volatility_proxy\"] <= vol_thresh].copy()\n",
        "    if dff.empty:\n",
        "        print(\"No coins within your risk tolerance.\"); return\n",
        "    header = f\"{'Name':<20} {'Sym':<6} {'VolProxy':>9} {col:>12}  Reason\"\n",
        "    print(\"\\nRecommended (within your volatility tolerance):\")\n",
        "    print(header); print(\"-\"*len(header))\n",
        "    for _, r in dff.sort_values(\"volatility_proxy\").iterrows():\n",
        "        print(f\"{str(r['name'])[:20]:<20} {r['symbol']:<6} \"\n",
        "              f\"{(r['volatility_proxy'] or 0):>9.2f} \"\n",
        "              f\"{(r.get(col) or 0):>12.2f}  \"\n",
        "              f\"{r['risk_reason']}\")\n",
        "\n",
        "# Example:\n",
        "show_risk_based_recommendations(results_df, risk_level=risk_level, time_horizon=\"24h\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVXs_h_LTGdD",
        "outputId": "2213c591-a40b-42e8-d24c-a8e124f13150"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== BUY (Trend) ===\n",
            "Empty DataFrame\n",
            "Columns: [name, symbol, current_price, price_change_percentage_24h, price_change_percentage_48h, price_change_percentage_72h, volatility_proxy, risk_score, risk_reason, recommend_score]\n",
            "Index: []\n",
            "\n",
            "=== BUY (Reversal) ===\n",
            "Empty DataFrame\n",
            "Columns: [name, symbol, current_price, price_change_percentage_24h, price_change_percentage_48h, price_change_percentage_72h, volatility_proxy, risk_score, risk_reason, recommend_score]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "# Cell 13 — rules-based buy recommendations (explainable demo)\n",
        "def recommend_coins(df: pd.DataFrame, strategy=\"trend\", top_k=10):\n",
        "    d = df.copy()\n",
        "    # ensure presence of needed columns\n",
        "    for col in [\"price_change_percentage_24h\",\"price_change_percentage_48h\",\n",
        "                \"price_change_percentage_72h\",\"volatility_proxy\",\"risk_score\"]:\n",
        "        if col not in d.columns: d[col] = np.nan\n",
        "\n",
        "    if strategy == \"trend\":\n",
        "        mask = (\n",
        "            (d[\"risk_score\"] < 65) &\n",
        "            (d[\"volatility_proxy\"] < d[\"volatility_proxy\"].quantile(0.7)) &\n",
        "            (d[\"price_change_percentage_24h\"] > 0) &\n",
        "            (d[\"price_change_percentage_48h\"] > 0) &\n",
        "            (d[\"price_change_percentage_72h\"] >= 0)\n",
        "        )\n",
        "        score = (0.5*d[\"price_change_percentage_24h\"] +\n",
        "                 0.3*d[\"price_change_percentage_48h\"] +\n",
        "                 0.2*d[\"price_change_percentage_72h\"] -\n",
        "                 0.2*d[\"volatility_proxy\"])\n",
        "\n",
        "    elif strategy == \"reversal\":\n",
        "        mask = (\n",
        "            (d[\"risk_score\"] < 55) &\n",
        "            (d[\"price_change_percentage_24h\"] < 0) &\n",
        "            (d[\"price_change_percentage_72h\"] > 0) &\n",
        "            (d[\"volatility_proxy\"] < d[\"volatility_proxy\"].quantile(0.8))\n",
        "        )\n",
        "        score = ((-1.0)*d[\"price_change_percentage_24h\"] +\n",
        "                 0.3*d[\"price_change_percentage_72h\"] -\n",
        "                 0.2*d[\"volatility_proxy\"])\n",
        "    else:\n",
        "        raise ValueError(\"strategy must be 'trend' or 'reversal'\")\n",
        "\n",
        "    out = d[mask].copy()\n",
        "    out[\"recommend_score\"] = score[mask]\n",
        "    out = out.sort_values(\"recommend_score\", ascending=False)\n",
        "    return out.head(top_k)[[\n",
        "        \"name\",\"symbol\",\"current_price\",\n",
        "        \"price_change_percentage_24h\",\"price_change_percentage_48h\",\"price_change_percentage_72h\",\n",
        "        \"volatility_proxy\",\"risk_score\",\"risk_reason\",\"recommend_score\"\n",
        "    ]]\n",
        "\n",
        "def show_recommendations(df: pd.DataFrame):\n",
        "    print(\"\\n=== BUY (Trend) ===\")\n",
        "    print(recommend_coins(df, \"trend\", top_k=10).to_string(index=False))\n",
        "    print(\"\\n=== BUY (Reversal) ===\")\n",
        "    print(recommend_coins(df, \"reversal\", top_k=10).to_string(index=False))\n",
        "\n",
        "# Example:\n",
        "show_recommendations(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoMB0-2kmMSv",
        "outputId": "5600f262-0412-4efd-949d-0300cc4cd202"
      },
      "outputs": [],
      "source": [
        "# Cell A — write polished Streamlit app\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write('''import os, json, time, pathlib, requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# -------- App config --------\n",
        "st.set_page_config(page_title=\"Crashcaster — Early Warning\", page_icon=\"💥\", layout=\"wide\")\n",
        "\n",
        "# --- UX polish (hide chrome, badges) ---\n",
        "HIDE_DEFAULT = \"\"\"\n",
        "<style>\n",
        "#MainMenu {visibility: hidden;}\n",
        "footer {visibility: hidden;}\n",
        "header {visibility: hidden;}\n",
        "section[data-testid=\"stSidebar\"] {background:#0E1117;}\n",
        "div.block-container {padding-top: 1.0rem;}\n",
        ".badge {display:inline-block;padding:6px 10px;border-radius:999px;font-weight:600;margin-right:6px;}\n",
        ".badge.green{background:#1f6f43;color:#fff;}\n",
        ".badge.orange{background:#a86b16;color:#fff;}\n",
        ".badge.red{background:#a11f2c;color:#fff;}\n",
        "</style>\n",
        "\"\"\"\n",
        "st.markdown(HIDE_DEFAULT, unsafe_allow_html=True)\n",
        "\n",
        "def risk_badge_class(x):\n",
        "    return \"red\" if x>=70 else (\"orange\" if x>=40 else \"green\")\n",
        "\n",
        "def reason_badges(reason_text: str):\n",
        "    parts = [p.strip() for p in (reason_text or \"\").split(\";\") if p.strip()]\n",
        "    html = \"\".join([f\"<span class='badge {'red' if ('High' in p or 'Large' in p) else 'green'}'>{p}</span>\" for p in parts])\n",
        "    return html or \"<span class='badge green'>Stable</span>\"\n",
        "\n",
        "# -------- Data config --------\n",
        "BASE_URL_CG = \"https://api.coingecko.com/api/v3\"\n",
        "CACHE_DIR   = pathlib.Path(\"./cache\");   CACHE_DIR.mkdir(exist_ok=True)\n",
        "OFFLINE_DIR = pathlib.Path(\"./offline\"); OFFLINE_DIR.mkdir(exist_ok=True)\n",
        "OFFLINE_FILE = OFFLINE_DIR / \"markets_sample.json\"\n",
        "TIMEOUT, REQUEST_DELAY, CACHE_TTL = 8, 2.0, 600\n",
        "\n",
        "# -------- Small utils --------\n",
        "def _cache_path(key): return CACHE_DIR / f\"{key}.json\"\n",
        "def _read_cache(key, ttl=CACHE_TTL):\n",
        "    p = _cache_path(key)\n",
        "    if not p.exists(): return None\n",
        "    if time.time() - p.stat().st_mtime > ttl: return None\n",
        "    try: return json.loads(p.read_text())\n",
        "    except: return None\n",
        "def _write_cache(key, data): _cache_path(key).write_text(json.dumps(data))\n",
        "\n",
        "def pct_change_from_sparkline(prices, hours):\n",
        "    if not prices or len(prices) <= hours: return None\n",
        "    last = float(prices[-1]); prev = float(prices[-hours-1])\n",
        "    if prev == 0: return None\n",
        "    return 100.0 * (last - prev) / prev\n",
        "\n",
        "# -------- Primary source (CoinGecko) --------\n",
        "def fetch_cg_markets(max_coins=30, currency=\"usd\", ttl=CACHE_TTL):\n",
        "    key=f\"cg_{currency}_{max_coins}_spark\"\n",
        "    cached = _read_cache(key, ttl=ttl)\n",
        "    if cached is not None:\n",
        "        return cached, \"CACHE(CG)\"\n",
        "    url=(f\"{BASE_URL_CG}/coins/markets?vs_currency={currency}\"\n",
        "         f\"&order=market_cap_desc&per_page={max_coins}&page=1\"\n",
        "         f\"&sparkline=true&price_change_percentage=1h,24h,7d,30d\")\n",
        "    r = requests.get(url, timeout=TIMEOUT, headers={\"Accept\":\"application/json\"})\n",
        "    if r.status_code != 200:\n",
        "        raise RuntimeError(f\"CG {r.status_code}\")\n",
        "    data = r.json()\n",
        "    _write_cache(key, data)\n",
        "    time.sleep(REQUEST_DELAY)\n",
        "    return data, \"LIVE(CG)\"\n",
        "\n",
        "def norm_from_cg(row):\n",
        "    return {\n",
        "        \"id\": row.get(\"id\"),\n",
        "        \"name\": row.get(\"name\"),\n",
        "        \"symbol\": (row.get(\"symbol\") or \"\").upper(),\n",
        "        \"current_price\": row.get(\"current_price\"),\n",
        "        \"market_cap\": row.get(\"market_cap\"),\n",
        "        \"volume_24h\": row.get(\"total_volume\"),\n",
        "        \"price_change_percentage_1h\":  row.get(\"price_change_percentage_1h_in_currency\", row.get(\"price_change_percentage_1h\")),\n",
        "        \"price_change_percentage_24h\": row.get(\"price_change_percentage_24h_in_currency\", row.get(\"price_change_percentage_24h\")),\n",
        "        \"price_change_percentage_7d\":  row.get(\"price_change_percentage_7d_in_currency\", row.get(\"price_change_percentage_7d\")),\n",
        "        \"price_change_percentage_30d\": row.get(\"price_change_percentage_30d_in_currency\", row.get(\"price_change_percentage_30d\")),\n",
        "        \"sparkline\": (row.get(\"sparkline\") or {}).get(\"price\", []),\n",
        "    }\n",
        "\n",
        "def load_offline_json():\n",
        "    if OFFLINE_FILE.exists():\n",
        "        try: return json.loads(OFFLINE_FILE.read_text())\n",
        "        except: return []\n",
        "    return []\n",
        "\n",
        "def get_markets_with_fallback(source=\"auto\", max_coins=30, currency=\"usd\"):\n",
        "    last_err = None\n",
        "    if source in (\"auto\",\"primary\"):\n",
        "        try:\n",
        "            rows, src = fetch_cg_markets(max_coins, currency)\n",
        "            norm = [norm_from_cg(r) for r in rows][:max_coins]\n",
        "            try: OFFLINE_FILE.write_text(json.dumps(norm, indent=2))\n",
        "            except: pass\n",
        "            df = pd.DataFrame(norm)\n",
        "            df[\"price_change_percentage_48h\"] = df[\"sparkline\"].apply(lambda p: pct_change_from_sparkline(p,48))\n",
        "            df[\"price_change_percentage_72h\"] = df[\"sparkline\"].apply(lambda p: pct_change_from_sparkline(p,72))\n",
        "            df[\"price_change_percentage_24h\"] = df.apply(\n",
        "                lambda r: r[\"price_change_percentage_24h\"] if pd.notna(r[\"price_change_percentage_24h\"])\n",
        "                else pct_change_from_sparkline(r[\"sparkline\"],24), axis=1)\n",
        "            return df, src, None\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "    raw = load_offline_json()\n",
        "    if raw:\n",
        "        df = pd.DataFrame(raw).head(max_coins)\n",
        "        if \"sparkline\" in df.columns:\n",
        "            df[\"price_change_percentage_48h\"] = df[\"sparkline\"].apply(lambda p: pct_change_from_sparkline(p,48))\n",
        "            df[\"price_change_percentage_72h\"] = df[\"sparkline\"].apply(lambda p: pct_change_from_sparkline(p,72))\n",
        "        return df, \"OFFLINE(JSON)\", last_err\n",
        "    raise RuntimeError(f\"No data sources available. Last live error: {last_err}\")\n",
        "\n",
        "# -------- Features + risk --------\n",
        "def _coerce_numeric(df):\n",
        "    for c in [\"current_price\",\"market_cap\",\"volume_24h\",\n",
        "              \"price_change_percentage_1h\",\"price_change_percentage_24h\",\n",
        "              \"price_change_percentage_48h\",\"price_change_percentage_72h\",\n",
        "              \"price_change_percentage_7d\",\"price_change_percentage_30d\"]:\n",
        "        if c in df.columns: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def _norm(s: pd.Series) -> pd.Series:\n",
        "    s2 = s.replace([np.inf,-np.inf], np.nan)\n",
        "    m, sd = s2.mean(), s2.std()\n",
        "    if not np.isfinite(sd) or sd == 0: return pd.Series(0, index=s.index)\n",
        "    z = (s2 - m) / sd\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "def build_features_and_risk(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = _coerce_numeric(df.copy())\n",
        "    df[\"abs_24h\"]      = df[\"price_change_percentage_24h\"].abs()\n",
        "    df[\"down_1h\"]      = df[\"price_change_percentage_1h\"].clip(upper=0).abs()\n",
        "    df[\"volume_ratio\"] = (df[\"volume_24h\"]/df[\"market_cap\"]).clip(lower=0, upper=1)\n",
        "    df[\"volatility_proxy\"] = (\n",
        "        df[\"price_change_percentage_1h\"].abs().fillna(0) +\n",
        "        df[\"price_change_percentage_24h\"].abs().fillna(0) +\n",
        "        df[\"price_change_percentage_7d\"].abs().fillna(0)\n",
        "    )/3.0\n",
        "    risk = (0.50*_norm(df[\"volatility_proxy\"]) +\n",
        "            0.30*_norm(df[\"abs_24h\"]) +\n",
        "            0.15*_norm(df[\"down_1h\"]) +\n",
        "            0.05*_norm(df[\"volume_ratio\"]))\n",
        "    df[\"risk_score\"] = (100*risk).clip(0,100).round(1)\n",
        "    # reasons\n",
        "    reasons=[]\n",
        "    for _, r in df.iterrows():\n",
        "        why=[]\n",
        "        if r[\"volatility_proxy\"] > df[\"volatility_proxy\"].median(): why.append(\"High recent volatility\")\n",
        "        if r[\"abs_24h\"] > df[\"abs_24h\"].median(): why.append(\"Large 24h move\")\n",
        "        if r[\"price_change_percentage_1h\"] < -0.5: why.append(\"1h downside\")\n",
        "        if r[\"volume_ratio\"] > 0.5: why.append(\"Heavy volume vs mcap\")\n",
        "        reasons.append(\"; \".join(why) if why else \"Stable / normal range\")\n",
        "    df[\"risk_reason\"] = reasons\n",
        "    return df\n",
        "\n",
        "# -------- Sidebar --------\n",
        "with st.sidebar:\n",
        "    st.header(\"Settings\")\n",
        "    source = st.selectbox(\"Data source\", [\"auto\",\"primary\",\"offline\"], index=0)\n",
        "    max_coins = st.slider(\"Coins\", 10, 100, 30)\n",
        "    threshold = st.slider(\"Risk threshold\", 0, 100, 70)\n",
        "\n",
        "    st.caption(\"Utilities\")\n",
        "    if st.button(\"Refresh offline snapshot\"):\n",
        "        try:\n",
        "            rows, _ = fetch_cg_markets(max_coins=max_coins, currency=\"usd\")\n",
        "            norm = [norm_from_cg(r) for r in rows][:max_coins]\n",
        "            OFFLINE_FILE.write_text(json.dumps(norm, indent=2))\n",
        "            st.success(\"Offline snapshot refreshed ✓\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Live fetch failed: {e}\")\n",
        "    if st.button(\"Clear API cache\"):\n",
        "        import pathlib\n",
        "        for p in pathlib.Path(\"cache\").glob(\"*.json\"): p.unlink()\n",
        "        st.success(\"Cache cleared.\")\n",
        "\n",
        "@st.cache_data(ttl=600, show_spinner=False)\n",
        "def load_and_score(src, n):\n",
        "    df, src_used, last_err = get_markets_with_fallback(source=src, max_coins=n)\n",
        "    df = build_features_and_risk(df).sort_values(\"risk_score\", ascending=False).reset_index(drop=True)\n",
        "    return df, src_used, last_err\n",
        "\n",
        "# -------- Main --------\n",
        "st.title(\"💥 Crashcaster — Early Warning for Crypto Crashes\")\n",
        "df, src_used, last_err = load_and_score(source, max_coins)\n",
        "\n",
        "# KPIs\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "if not df.empty:\n",
        "    top = df.iloc[0]\n",
        "    col1.metric(\"Top Risk Coin\", f\"{top['symbol']}\", f\"{top['risk_score']:.0f}\")\n",
        "    col2.metric(\"Market Risk Avg\", f\"{df['risk_score'].mean():.0f}%\")\n",
        "    col3.metric(\"Coins ≥ Threshold\", f\"{(df['risk_score']>=threshold).sum()}/{len(df)}\")\n",
        "    col4.metric(\"Data Source\", src_used)\n",
        "\n",
        "if src_used.startswith(\"OFFLINE\") and last_err:\n",
        "    st.warning(f\"Using offline snapshot. Live fetch failed with: {last_err}\")\n",
        "\n",
        "# -------- Hero “Try it now” panel --------\n",
        "st.markdown(\"### Try it now\")\n",
        "left, right = st.columns([3,1])\n",
        "with left:\n",
        "    symbols = df[\"symbol\"].dropna().unique().tolist()\n",
        "    symbols.sort()\n",
        "    selected_symbol = st.selectbox(\"Pick a coin\", symbols, index=0)\n",
        "with right:\n",
        "    analyze_clicked = st.button(\"Analyze\", use_container_width=True)\n",
        "\n",
        "if analyze_clicked:\n",
        "    st.session_state[\"selected_symbol\"] = selected_symbol\n",
        "\n",
        "if \"selected_symbol\" in st.session_state:\n",
        "    sel = st.session_state[\"selected_symbol\"]\n",
        "    row = df[df[\"symbol\"] == sel].head(1)\n",
        "    if not row.empty:\n",
        "        r = row.iloc[0]\n",
        "        c1, c2 = st.columns([2,1])\n",
        "        with c1:\n",
        "            st.subheader(f\"{r['name']} ({r['symbol']})\")\n",
        "            spark = r.get(\"sparkline\", None)\n",
        "            if isinstance(spark, (list, tuple)) and len(spark) > 5:\n",
        "                fig = go.Figure(go.Scatter(y=spark, mode=\"lines\"))\n",
        "                fig.update_layout(height=200, margin=dict(l=10,r=10,t=10,b=10), showlegend=False)\n",
        "                st.plotly_chart(fig, use_container_width=True)\n",
        "            s24 = r.get(\"price_change_percentage_24h\", 0) or 0\n",
        "            s48 = r.get(\"price_change_percentage_48h\", np.nan)\n",
        "            s72 = r.get(\"price_change_percentage_72h\", np.nan)\n",
        "            st.markdown(\n",
        "                f\"**24h:** {s24:+.2f}% · **48h:** {s48 if pd.notna(s48) else '—'}% · \"\n",
        "                f\"**72h:** {s72 if pd.notna(s72) else '—'}%\"\n",
        "            )\n",
        "        with c2:\n",
        "            gauge = go.Figure(go.Indicator(mode=\"gauge+number\", value=float(r[\"risk_score\"]),\n",
        "                                           gauge={\"axis\":{\"range\":[0,100]}}, title={\"text\":\"Risk score\"}))\n",
        "            gauge.update_layout(height=220, margin=dict(l=10,r=10,t=30,b=10))\n",
        "            st.plotly_chart(gauge, use_container_width=True)\n",
        "            st.markdown(f\"<div>{reason_badges(r['risk_reason'])}</div>\", unsafe_allow_html=True)\n",
        "\n",
        "# -------- Tabs --------\n",
        "tab1, tab2 = st.tabs([\"📊 Dashboard\",\"✅ Recommendations\"])\n",
        "\n",
        "with tab1:\n",
        "    st.subheader(\"At-Risk Coins\")\n",
        "    danger = df[df[\"risk_score\"]>=threshold]\n",
        "    st.dataframe(danger[[\n",
        "        \"name\",\"symbol\",\"current_price\",\n",
        "        \"price_change_percentage_24h\",\"price_change_percentage_48h\",\"price_change_percentage_72h\",\n",
        "        \"price_change_percentage_7d\",\n",
        "        \"volatility_proxy\",\"volume_24h\",\"market_cap\",\"risk_score\",\"risk_reason\"\n",
        "    ]], use_container_width=True)\n",
        "\n",
        "    st.subheader(\"All Coins\")\n",
        "    st.dataframe(df[[\n",
        "        \"name\",\"symbol\",\"current_price\",\n",
        "        \"price_change_percentage_24h\",\"price_change_percentage_48h\",\"price_change_percentage_72h\",\n",
        "        \"price_change_percentage_7d\",\n",
        "        \"volatility_proxy\",\"volume_24h\",\"market_cap\",\"risk_score\",\"risk_reason\"\n",
        "    ]], use_container_width=True)\n",
        "\n",
        "    st.subheader(\"Charts\")\n",
        "    top_risk = df.nlargest(10, \"risk_score\")\n",
        "    fig1 = px.bar(top_risk, x=\"symbol\", y=\"risk_score\", title=\"Top-10 Crash Risk\")\n",
        "    st.plotly_chart(fig1, use_container_width=True)\n",
        "    fig2 = go.Figure()\n",
        "    fig2.add_bar(name=\"24h %\", x=df[\"symbol\"], y=df[\"price_change_percentage_24h\"])\n",
        "    fig2.add_bar(name=\"48h %\", x=df[\"symbol\"], y=df[\"price_change_percentage_48h\"])\n",
        "    fig2.add_bar(name=\"72h %\", x=df[\"symbol\"], y=df[\"price_change_percentage_72h\"])\n",
        "    fig2.update_layout(barmode=\"group\", title=\"Price Change: 24/48/72h\")\n",
        "    st.plotly_chart(fig2, use_container_width=True)\n",
        "\n",
        "with tab2:\n",
        "    def recommend_coins_local(dff, strategy=\"trend\", top_k=10):\n",
        "        d = dff.copy()\n",
        "        for col in [\"price_change_percentage_24h\",\"price_change_percentage_48h\",\n",
        "                    \"price_change_percentage_72h\",\"volatility_proxy\",\"risk_score\"]:\n",
        "            if col not in d.columns: d[col] = np.nan\n",
        "            d[col] = pd.to_numeric(d[col], errors=\"coerce\")\n",
        "        c24 = d[\"price_change_percentage_24h\"].fillna(0)\n",
        "        c48 = d[\"price_change_percentage_48h\"].fillna(0)\n",
        "        c72 = d[\"price_change_percentage_72h\"].fillna(0)\n",
        "        vol = d[\"volatility_proxy\"].fillna(d[\"volatility_proxy\"].median())\n",
        "        def ok_nonneg_or_nan(s): return (s >= 0) | (s.isna())\n",
        "        if strategy == \"trend\":\n",
        "            mask = ((d[\"risk_score\"] < 65) & (vol < vol.quantile(0.7)) &\n",
        "                    (c24 > 0) & ok_nonneg_or_nan(d[\"price_change_percentage_48h\"]) &\n",
        "                    ok_nonneg_or_nan(d[\"price_change_percentage_72h\"]))\n",
        "            score = (0.60*c24 + 0.25*c48 + 0.15*c72 - 0.20*vol)\n",
        "        else:  # reversal\n",
        "            mask = ((d[\"risk_score\"] < 55) & (c24 < 0) &\n",
        "                    ok_nonneg_or_nan(d[\"price_change_percentage_72h\"]) &\n",
        "                    (vol < vol.quantile(0.8)))\n",
        "            score = ((-1.0)*c24 + 0.30*c72 - 0.20*vol)\n",
        "        out = d[mask].copy()\n",
        "        out[\"recommend_score\"] = score[mask]\n",
        "        out = out.sort_values(\"recommend_score\", ascending=False).head(top_k)\n",
        "        if out.empty:\n",
        "            fb = d[(d[\"risk_score\"] < 60)].copy()\n",
        "            fb[\"recommend_score\"] = 0.70*c24 - 0.20*vol\n",
        "            out = fb.sort_values(\"recommend_score\", ascending=False).head(top_k)\n",
        "            st.info(\"Showing fallback list (limited 48/72h data). Refresh the offline snapshot when online to improve results.\")\n",
        "        cols = [\"name\",\"symbol\",\"current_price\",\n",
        "                \"price_change_percentage_24h\",\"price_change_percentage_48h\",\"price_change_percentage_72h\",\n",
        "                \"volatility_proxy\",\"risk_score\",\"risk_reason\",\"recommend_score\"]\n",
        "        return out[cols]\n",
        "\n",
        "    strat = st.selectbox(\"Strategy\", [\"trend\",\"reversal\"], index=0)\n",
        "    rec = recommend_coins_local(df, strat, top_k=10)\n",
        "    st.dataframe(rec, use_container_width=True)\n",
        "\n",
        "st.caption(\"Tip: If the API hiccups, create an offline snapshot (sidebar) and set Data source → 'offline'. Not financial advice.\")\n",
        "''')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68lAkXuZYfQc",
        "outputId": "7479b46f-5472-4038-d122-a1ff728dce0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://9aeb9ffdd2e4.ngrok-free.app\n",
            "Streamlit PID: 14057\n",
            "Tail logs with:  !tail -n 200 logs/streamlit.log\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "t=2025-08-14T22:59:13-0400 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n"
          ]
        }
      ],
      "source": [
        "# ✅ Start Streamlit (subprocess), wait for port 8501, then open ngrok tunnel\n",
        "\n",
        "import sys, subprocess, shlex, time, socket, pathlib\n",
        "from contextlib import closing\n",
        "\n",
        "PORT = 8501\n",
        "NGROK_AUTHTOKEN = \"31IxW8d8joEMdtLFoWy59u5f8Fb_V1gsdmuc6T4RY13aHW8P\"  # rotate later\n",
        "\n",
        "# 0) Ensure deps\n",
        "try:\n",
        "    import streamlit  # noqa\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"streamlit\"])\n",
        "try:\n",
        "    from pyngrok import ngrok, conf\n",
        "except ModuleNotFoundError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pyngrok\"])\n",
        "    from pyngrok import ngrok, conf\n",
        "\n",
        "# 1) Kill any old ngrok agent/tunnels (avoid “too many tunnels” / stale sessions)\n",
        "try:\n",
        "    for t in ngrok.get_tunnels():\n",
        "        ngrok.disconnect(t.public_url)\n",
        "    ngrok.kill()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# 2) Launch Streamlit as a subprocess (bind to 0.0.0.0 so ngrok can reach it)\n",
        "pathlib.Path(\"logs\").mkdir(exist_ok=True)\n",
        "log_f = open(\"logs/streamlit.log\", \"w\")\n",
        "cmd = f'{sys.executable} -m streamlit run app.py --server.port {PORT} --server.address 0.0.0.0 --server.headless true'\n",
        "proc = subprocess.Popen(shlex.split(cmd), stdout=log_f, stderr=subprocess.STDOUT)\n",
        "\n",
        "# Store handles so you can stop later\n",
        "globals()[\"_STREAMLIT_PROC\"] = proc\n",
        "globals()[\"_STREAMLIT_LOG\"] = log_f\n",
        "\n",
        "# 3) Wait until the port is actually accepting connections\n",
        "def wait_for_port(port, host=\"127.0.0.1\", timeout=60):\n",
        "    t0 = time.time()\n",
        "    while time.time() - t0 < timeout:\n",
        "        with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:\n",
        "            sock.settimeout(1.0)\n",
        "            if sock.connect_ex((host, port)) == 0:\n",
        "                return True\n",
        "        time.sleep(0.5)\n",
        "    return False\n",
        "\n",
        "ok = wait_for_port(PORT)\n",
        "if not ok:\n",
        "    print(\"Streamlit failed to start on port\", PORT)\n",
        "    print(\"Check logs with: !tail -n 200 logs/streamlit.log\")\n",
        "else:\n",
        "    # 4) Open ngrok tunnel only after Streamlit is up\n",
        "    conf.get_default().auth_token = NGROK_AUTHTOKEN\n",
        "    tunnel = ngrok.connect(PORT)\n",
        "    globals()[\"_NGROK_TUNNEL\"] = tunnel\n",
        "    print(\"Public URL:\", tunnel.public_url)\n",
        "    print(\"Streamlit PID:\", proc.pid)\n",
        "    print(\"Tail logs with:  !tail -n 200 logs/streamlit.log\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
